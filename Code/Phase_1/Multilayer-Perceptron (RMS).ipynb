{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeded_matrix(dataAddress='./../Data/eng.train',frq = 2,debug=0):\n",
    "    if( debug == 1 ):\n",
    "        print(\"---------inside create_embeded_matrix function---------\\n\")\n",
    "        \n",
    "    assert(debug==0 or debug ==1)\n",
    "    \n",
    "    with open(dataAddress,'r') as data:\n",
    "        line = data.readlines()\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"Total data : %d\"%(len(line)))\n",
    "    \n",
    "    isO   = {}\n",
    "    all_X = []\n",
    "    all_Y = []\n",
    "    cnt = 0 ; \n",
    "    for i in line:\n",
    "        temp = i.split()\n",
    "        l = len(temp)     \n",
    "        if( l != 4 ):\n",
    "            continue\n",
    "        if( temp[3] == \"O\" ):\n",
    "            cnt = cnt+1 ; \n",
    "        all_X.append(temp[0]) ;\n",
    "        if( temp[0] not in isO ):\n",
    "            isO[ temp[0] ]  = 0 \n",
    "        if( temp[3] != \"O\" ):\n",
    "            isO[ temp[0] ]  = 1 \n",
    "    \n",
    "    if( debug == 1):\n",
    "        print(\"Total Number of **Other** NE =\",cnt)\n",
    "        init_reduce = len(line)-len(all_X) \n",
    "        print(\"Initial reduction : %d\"%(init_reduce))\n",
    "        print(\"Current total : %d\"%len(all_X))\n",
    "        \n",
    "    \n",
    "    \n",
    "    frqCnt=dict()\n",
    "    for i in all_X :\n",
    "        if( i not in frqCnt ):\n",
    "            frqCnt[i] = 1\n",
    "        else: \n",
    "            frqCnt[i] = frqCnt[i]+1\n",
    "            \n",
    "    if( debug == 1):\n",
    "        print(\"Size of the dictionary(Unique Words) : %d\"%len(frqCnt))\n",
    "        p = set(all_X) ;\n",
    "        assert( len(p) == len(frqCnt) )\n",
    "    \n",
    "    \n",
    "    shrinked_X = []\n",
    "    for k,v in frqCnt.items():\n",
    "        if( (v >= frq) and (isO[k] == 1) ):\n",
    "            shrinked_X.append(k) \n",
    "    shrinked_X.append(\"*?*other\")\n",
    "    \n",
    "    assert(len(shrinked_X) == len(set(shrinked_X)))\n",
    "    \n",
    "    em_hash = dict()\n",
    "    idx = 0 \n",
    "    for i in shrinked_X:\n",
    "        if i not in em_hash:\n",
    "            em_hash[i] = idx\n",
    "        idx = idx + 1\n",
    "    \n",
    "    if( debug == 1):\n",
    "        print(\"Total item after shrinking by frequency-{0} : {1}\".format(frq, len(shrinked_X)))\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"\\n---------end of create_embeded_matrix function---------\")\n",
    "    \n",
    "    return (shrinked_X,em_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_map(debug=0):\n",
    "    \"\"\"\n",
    "        Category defination\n",
    "        O = others\n",
    "        I/B = 2\n",
    "        LOC/MISC/ORG/PER = 4\n",
    "        Total number of Combination = 1+2*4 = 9\n",
    "    \"\"\"\n",
    "    if( debug == 1 ):\n",
    "        print(\"---------inside output_map function---------\\n\")\n",
    "    assert(debug ==0 or debug == 1)\n",
    "    \n",
    "    IB=[\"I\",\"B\"]\n",
    "    ST=[\"LOC\",\"MISC\",\"ORG\",\"PER\"]\n",
    "    output_type = []\n",
    "    for i in IB:\n",
    "        for j in ST:\n",
    "            output_type.append(i+\"-\"+j)\n",
    "    output_type.append(\"O\")\n",
    "    \n",
    "    ret = dict()\n",
    "    idx = 0 \n",
    "    for i in output_type:\n",
    "        ret[i] = idx\n",
    "        if( debug == 1 ):\n",
    "             print(i+\"'s index number is %d\"%idx )\n",
    "        idx = idx+1\n",
    "    if( debug == 1 ):\n",
    "        print(\"\\n---------end of inside output_map function---------\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def parse_input(dataAddress='./../Data/eng.train',debug=0):\n",
    "    if( debug == 1 ):\n",
    "        print(\"---------inside parse function---------\\n\")\n",
    "        \n",
    "    assert(debug==0 or debug ==1)\n",
    "    \n",
    "    with open(dataAddress,'r') as data:\n",
    "        line = data.readlines()\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"Total data : %d\"%(len(line)))\n",
    "    \n",
    "\n",
    "    all_X = []\n",
    "    all_Y = []\n",
    "    for i in line:\n",
    "        temp = i.split()\n",
    "        l = len(temp) \n",
    "        if( l != 4 ):\n",
    "            continue\n",
    "        #idx = output_hash[temp[3]]\n",
    "        #temp_one_hot_Y = np.zeros(len(output_hash));\n",
    "        #temp_one_hot_Y[idx] = 1 ;\n",
    "        \n",
    "        all_X.append(temp[0]) ;\n",
    "        all_Y.append(temp[3]) ;\n",
    "    \n",
    " \n",
    "    if( debug == 1):\n",
    "        init_reduce = len(line)-len(all_X) \n",
    "        print(\"Initial reduction : %d\"%(init_reduce))\n",
    "        print(\"Current total : %d\"%len(all_X))\n",
    "        assert(len(all_X)==len(all_Y))\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"\\n---------end of parse function---------\")\n",
    "    return all_X,all_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# information from datasets\n",
    "\n",
    "## eng.testa\n",
    "-----------\n",
    "+ I-LOC\n",
    "+ B-MISC\n",
    "+ O\n",
    "+ I-MISC\n",
    "+ I-PER\n",
    "+ I-ORG\n",
    "\n",
    "## eng.testb\n",
    "+ I-LOC\n",
    "+ B-MISC\n",
    "+ B-LOC\n",
    "+ O\n",
    "+ I-MISC\n",
    "+ I-PER\n",
    "+ I-ORG\n",
    "\n",
    "## eng.train\n",
    "+ I-LOC\n",
    "+ I-MISC\n",
    "+ I-PER\n",
    "+ I-ORG\n",
    "+ B-LOC\n",
    "+ B-MISC\n",
    "+ B-ORG\n",
    "+ O\n",
    "\n",
    "## temporary archived code\n",
    "\"\"\"\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RND_SEED = 100\n",
    "tf.set_random_seed(RND_SEED)\n",
    "\n",
    "\n",
    "    Embeded Matrix\n",
    "    EM = create_embeded_matrix(debug=1)\n",
    "    EM_X = EM[0] \n",
    "    EM_hash = EM[1]\n",
    "    take all the inputs from the eng.train. \n",
    "    output is represented in one hot encoding format\n",
    "    train_X, train_y = parse_input(debug=1)\n",
    "    number_of_softmax_layer = len(output_map())\n",
    "    \n",
    "\n",
    "    #design model\n",
    "    size_of_embeded_row = 100\n",
    "    for i in range(total_no_of_word):\n",
    "        W[i] = tf.Variable(tf.zeros([size_of_embeded_row = 100,number_of_softmax_layer]))\n",
    "        x[i] = tf.placeholder(tf.float32, [1, 100])\n",
    "        y += W[i]*x[i] ;\n",
    "\n",
    "    vocabulary_size = len(EM_X)\n",
    "    embedding_size = 100 \n",
    "    word_embeddings = tf.get_variable(\"word_embeddings\",[vocabulary_size, embedding_size])\n",
    "    \n",
    "    batch_size = len(EM_X)\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 9])\n",
    "    \n",
    "    embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, train_inputs)\n",
    "    print(embedded_word_ids.shape)\n",
    "    \n",
    "    nce_weights = tf.Variable(\n",
    "                      tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    num_sampled = batch_size;\n",
    "    init_loss = tf.reduce_mean(\n",
    "                 tf.nn.nce_loss(weights=nce_weights,\n",
    "                 biases=nce_biases,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embedded_word_ids,\n",
    "                 num_sampled=9,\n",
    "                 num_classes=vocabulary_size))\n",
    "    loss = tf.nn.softmax(init_loss)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.5).minimize(loss)\n",
    "    \n",
    "    feed_dict = {train_inputs:range(len(train_X)) ,def main(): train_labels: train_Y}\n",
    "    _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_input(batch_no,train_X,train_Y,batch_size=100):\n",
    "    row_st = batch_no*batch_size\n",
    "    if( row_st > len(train_X) ):\n",
    "        return [],[]\n",
    "    return train_X[row_st:min(row_st+batch_size,len(train_X)+1)],train_Y[row_st:min(row_st+batch_size,len(train_Y)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(vector,one_hot_hash):\n",
    "    retVec = []\n",
    "    for idx,val in enumerate(vector):\n",
    "        tmp_one_hot = np.zeros(len(one_hot_hash))\n",
    "        if val in one_hot_hash:\n",
    "            tmp_one_hot[ one_hot_hash[val] ] = 1 \n",
    "        else:\n",
    "            tmp_one_hot[ -1 ] = 1 \n",
    "        retVec.append(tmp_one_hot)\n",
    "    return retVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# With SGD/RMSProp/ADAM Training algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "RND_SEED = 100\n",
    "tf.set_random_seed(RND_SEED)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "        Hyper parameter list\n",
    "        \n",
    "        + EMBEDDING_DIM\n",
    "               \n",
    "    \"\"\"\n",
    "    \n",
    "    #Embeded Matrix\n",
    "    EM = create_embeded_matrix(debug=1)\n",
    "    EM_X = EM[0] \n",
    "    EM_hash = EM[1]\n",
    "    #print(EM_hash['EU'])\n",
    "    \n",
    "    #take all the inputs from the eng.train. \n",
    "    #output is represented in one hot encoding format\n",
    "    train_X, train_Y = parse_input(debug=1)\n",
    "    \n",
    "    train_X = np.asarray(train_X)\n",
    "    train_Y = np.asarray(train_Y)\n",
    "    \n",
    "    #print(train_X[1:10])\n",
    "    #print(train_Y[1:10])\n",
    "    \n",
    "    Y_hash = output_map()\n",
    "    output_size = len(Y_hash)\n",
    "    \n",
    "    #print(train_X.shape,train_Y.shape)\n",
    "\n",
    "    vocabulary_size = len(EM_X)\n",
    "    x = tf.placeholder(tf.float32,shape=[None,vocabulary_size])\n",
    "    y_goal = tf.placeholder(tf.float32, shape=[None,output_size])\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([vocabulary_size,EMBEDDING_DIM]))\n",
    "    b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "    hidden_representation = tf.add(tf.matmul(x,W1),b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM,output_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([output_size]))\n",
    "    prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_representation,W2),b2))\n",
    "    \n",
    "    cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_goal*tf.log(prediction),reduction_indices=[1]))\n",
    "    \n",
    "    #train_stepSGD = tf.train.GradientDescentOptimizer(.00001).minimize(cross_entropy_loss)\n",
    "    train_stepRMS = tf.train.RMSPropOptimizer(.0001).minimize(cross_entropy_loss)\n",
    "    #train_stepADAM = tf.train.AdamOptimizer(learning_rate=.0001).minimize(cross_entropy_loss)\n",
    "\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    epoch = 500\n",
    "    \n",
    "    \n",
    "    print(\"\\nEntering training Session\")\n",
    "    print(\"-------------------------\")\n",
    "    for itr in range(epoch):\n",
    "        batch_no = 0  \n",
    "        a = -1\n",
    "        b = -1 \n",
    "        while( 1 ):\n",
    "            curr_X, curr_Y = get_batch_input(batch_no,train_X,train_Y,batch_size=100)\n",
    "            if( len(curr_X) == 0 ):\n",
    "                break\n",
    "            curr_X = one_hot_embedding(curr_X,EM_hash)\n",
    "            curr_Y = one_hot_embedding(curr_Y,Y_hash)\n",
    "            curr_X = np.asarray(curr_X)\n",
    "            curr_Y = np.asarray(curr_Y)\n",
    "            #a, b = sess.run([train_stepSGD, cross_entropy_loss],feed_dict={x:curr_X,y_goal:curr_Y})\n",
    "            a, b = sess.run([train_stepRMS, cross_entropy_loss],feed_dict={x:curr_X,y_goal:curr_Y})\n",
    "            #a, b = sess.run([train_stepADAM, cross_entropy_loss],feed_dict={x:curr_X,y_goal:curr_Y})\n",
    "            batch_no = batch_no + 1 \n",
    "            #print(b)\n",
    "        #print(\"{0} epoch completed. current SGD loss {1}\".format(itr+1,b))\n",
    "        print(\"{0} epoch completed. current RMS loss {1}\".format(itr+1,b))\n",
    "        #print(\"{0} epoch completed. current ADAM loss {1}\".format(itr+1,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment taken on :  Tue Oct 17 17:20:41 2017\n",
      "---------inside create_embeded_matrix function---------\n",
      "\n",
      "Total data : 219553\n",
      "Total Number of **Other** NE = 170524\n",
      "Initial reduction : 14986\n",
      "Current total : 204567\n",
      "Size of the dictionary(Unique Words) : 23624\n",
      "Total item after shrinking by frequency-2 : 4575\n",
      "\n",
      "---------end of create_embeded_matrix function---------\n",
      "---------inside parse function---------\n",
      "\n",
      "Total data : 219553\n",
      "Initial reduction : 14986\n",
      "Current total : 204567\n",
      "\n",
      "---------end of parse function---------\n",
      "\n",
      "Entering training Session\n",
      "-------------------------\n",
      "1 epoch completed. current RMS loss 4.713581085205078\n",
      "2 epoch completed. current RMS loss 3.8941619396209717\n",
      "3 epoch completed. current RMS loss 3.4585413932800293\n",
      "4 epoch completed. current RMS loss 3.0551810264587402\n",
      "5 epoch completed. current RMS loss 2.645879030227661\n",
      "6 epoch completed. current RMS loss 2.267488718032837\n",
      "7 epoch completed. current RMS loss 1.968588948249817\n",
      "8 epoch completed. current RMS loss 1.794314980506897\n",
      "9 epoch completed. current RMS loss 1.6824617385864258\n",
      "10 epoch completed. current RMS loss 1.5788662433624268\n",
      "11 epoch completed. current RMS loss 1.4801570177078247\n",
      "12 epoch completed. current RMS loss 1.3878744840621948\n",
      "13 epoch completed. current RMS loss 1.3043098449707031\n",
      "14 epoch completed. current RMS loss 1.23117196559906\n",
      "15 epoch completed. current RMS loss 1.1681630611419678\n",
      "16 epoch completed. current RMS loss 1.1131960153579712\n",
      "17 epoch completed. current RMS loss 1.0642147064208984\n",
      "18 epoch completed. current RMS loss 1.019544005393982\n",
      "19 epoch completed. current RMS loss 0.9779108762741089\n",
      "20 epoch completed. current RMS loss 0.9385481476783752\n",
      "21 epoch completed. current RMS loss 0.9011551737785339\n",
      "22 epoch completed. current RMS loss 0.8654766082763672\n",
      "23 epoch completed. current RMS loss 0.8313425183296204\n",
      "24 epoch completed. current RMS loss 0.7986380457878113\n",
      "25 epoch completed. current RMS loss 0.7673079371452332\n",
      "26 epoch completed. current RMS loss 0.7374691963195801\n",
      "27 epoch completed. current RMS loss 0.7091099619865417\n",
      "28 epoch completed. current RMS loss 0.6821884512901306\n",
      "29 epoch completed. current RMS loss 0.6566706895828247\n",
      "30 epoch completed. current RMS loss 0.6325356960296631\n",
      "31 epoch completed. current RMS loss 0.6097412109375\n",
      "32 epoch completed. current RMS loss 0.58826744556427\n",
      "33 epoch completed. current RMS loss 0.5680571794509888\n",
      "34 epoch completed. current RMS loss 0.5490155220031738\n",
      "35 epoch completed. current RMS loss 0.5310614109039307\n",
      "36 epoch completed. current RMS loss 0.5141305327415466\n",
      "37 epoch completed. current RMS loss 0.4981711804866791\n",
      "38 epoch completed. current RMS loss 0.4831339120864868\n",
      "39 epoch completed. current RMS loss 0.46896442770957947\n",
      "40 epoch completed. current RMS loss 0.4556120038032532\n",
      "41 epoch completed. current RMS loss 0.443024605512619\n",
      "42 epoch completed. current RMS loss 0.43115749955177307\n",
      "43 epoch completed. current RMS loss 0.41996535658836365\n",
      "44 epoch completed. current RMS loss 0.40940651297569275\n",
      "45 epoch completed. current RMS loss 0.3994389772415161\n",
      "46 epoch completed. current RMS loss 0.39002618193626404\n",
      "47 epoch completed. current RMS loss 0.3811323642730713\n",
      "48 epoch completed. current RMS loss 0.3727246820926666\n",
      "49 epoch completed. current RMS loss 0.36477574706077576\n",
      "50 epoch completed. current RMS loss 0.35725486278533936\n",
      "51 epoch completed. current RMS loss 0.3501429259777069\n",
      "52 epoch completed. current RMS loss 0.34342068433761597\n",
      "53 epoch completed. current RMS loss 0.3370612859725952\n",
      "54 epoch completed. current RMS loss 0.3310409486293793\n",
      "55 epoch completed. current RMS loss 0.32534143328666687\n",
      "56 epoch completed. current RMS loss 0.3199467658996582\n",
      "57 epoch completed. current RMS loss 0.3148457109928131\n",
      "58 epoch completed. current RMS loss 0.3100202977657318\n",
      "59 epoch completed. current RMS loss 0.30545270442962646\n",
      "60 epoch completed. current RMS loss 0.3011317849159241\n",
      "61 epoch completed. current RMS loss 0.2970431447029114\n",
      "62 epoch completed. current RMS loss 0.29317614436149597\n",
      "63 epoch completed. current RMS loss 0.28951555490493774\n",
      "64 epoch completed. current RMS loss 0.2860528230667114\n",
      "65 epoch completed. current RMS loss 0.282778799533844\n",
      "66 epoch completed. current RMS loss 0.2796904444694519\n",
      "67 epoch completed. current RMS loss 0.27678075432777405\n",
      "68 epoch completed. current RMS loss 0.27403753995895386\n",
      "69 epoch completed. current RMS loss 0.27145153284072876\n",
      "70 epoch completed. current RMS loss 0.2690156102180481\n",
      "71 epoch completed. current RMS loss 0.2667241096496582\n",
      "72 epoch completed. current RMS loss 0.26456719636917114\n",
      "73 epoch completed. current RMS loss 0.26253876090049744\n",
      "74 epoch completed. current RMS loss 0.2606368362903595\n",
      "75 epoch completed. current RMS loss 0.25885123014450073\n",
      "76 epoch completed. current RMS loss 0.25717756152153015\n",
      "77 epoch completed. current RMS loss 0.25561094284057617\n",
      "78 epoch completed. current RMS loss 0.25415074825286865\n",
      "79 epoch completed. current RMS loss 0.25279223918914795\n",
      "80 epoch completed. current RMS loss 0.25153329968452454\n",
      "81 epoch completed. current RMS loss 0.25036802887916565\n",
      "82 epoch completed. current RMS loss 0.249292254447937\n",
      "83 epoch completed. current RMS loss 0.24830542504787445\n",
      "84 epoch completed. current RMS loss 0.24740512669086456\n",
      "85 epoch completed. current RMS loss 0.24658825993537903\n",
      "86 epoch completed. current RMS loss 0.2458474189043045\n",
      "87 epoch completed. current RMS loss 0.24518416821956635\n",
      "88 epoch completed. current RMS loss 0.2445909082889557\n",
      "89 epoch completed. current RMS loss 0.2440662682056427\n",
      "90 epoch completed. current RMS loss 0.24360422790050507\n",
      "91 epoch completed. current RMS loss 0.24320252239704132\n",
      "92 epoch completed. current RMS loss 0.24285362660884857\n",
      "93 epoch completed. current RMS loss 0.2425573319196701\n",
      "94 epoch completed. current RMS loss 0.24230925738811493\n",
      "95 epoch completed. current RMS loss 0.24210622906684875\n",
      "96 epoch completed. current RMS loss 0.24194899201393127\n",
      "97 epoch completed. current RMS loss 0.24183714389801025\n",
      "98 epoch completed. current RMS loss 0.24176768958568573\n",
      "99 epoch completed. current RMS loss 0.24173879623413086\n",
      "100 epoch completed. current RMS loss 0.2417444884777069\n",
      "101 epoch completed. current RMS loss 0.24178741872310638\n",
      "102 epoch completed. current RMS loss 0.24186837673187256\n",
      "103 epoch completed. current RMS loss 0.24198532104492188\n",
      "104 epoch completed. current RMS loss 0.2421395629644394\n",
      "105 epoch completed. current RMS loss 0.24232952296733856\n",
      "106 epoch completed. current RMS loss 0.2425519973039627\n",
      "107 epoch completed. current RMS loss 0.24280855059623718\n",
      "108 epoch completed. current RMS loss 0.2430998682975769\n",
      "109 epoch completed. current RMS loss 0.2434346228837967\n",
      "110 epoch completed. current RMS loss 0.24381791055202484\n",
      "111 epoch completed. current RMS loss 0.24425500631332397\n",
      "112 epoch completed. current RMS loss 0.24474921822547913\n",
      "113 epoch completed. current RMS loss 0.24530638754367828\n",
      "114 epoch completed. current RMS loss 0.24592314660549164\n",
      "115 epoch completed. current RMS loss 0.24660472571849823\n",
      "116 epoch completed. current RMS loss 0.24734950065612793\n",
      "117 epoch completed. current RMS loss 0.2481580376625061\n",
      "118 epoch completed. current RMS loss 0.24902363121509552\n",
      "119 epoch completed. current RMS loss 0.2499331533908844\n",
      "120 epoch completed. current RMS loss 0.25086718797683716\n",
      "121 epoch completed. current RMS loss 0.2518152892589569\n",
      "122 epoch completed. current RMS loss 0.25275829434394836\n",
      "123 epoch completed. current RMS loss 0.2536860406398773\n",
      "124 epoch completed. current RMS loss 0.2545856833457947\n",
      "125 epoch completed. current RMS loss 0.2554530203342438\n",
      "126 epoch completed. current RMS loss 0.2562795579433441\n",
      "127 epoch completed. current RMS loss 0.2570614516735077\n",
      "128 epoch completed. current RMS loss 0.25779736042022705\n",
      "129 epoch completed. current RMS loss 0.2584899067878723\n",
      "130 epoch completed. current RMS loss 0.2591383159160614\n",
      "131 epoch completed. current RMS loss 0.25973668694496155\n",
      "132 epoch completed. current RMS loss 0.26029255986213684\n",
      "133 epoch completed. current RMS loss 0.260795921087265\n",
      "134 epoch completed. current RMS loss 0.26125338673591614\n",
      "135 epoch completed. current RMS loss 0.2616610527038574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 epoch completed. current RMS loss 0.2620222270488739\n",
      "137 epoch completed. current RMS loss 0.26234304904937744\n",
      "138 epoch completed. current RMS loss 0.2626311480998993\n",
      "139 epoch completed. current RMS loss 0.26287832856178284\n",
      "140 epoch completed. current RMS loss 0.2630859911441803\n",
      "141 epoch completed. current RMS loss 0.2632542848587036\n",
      "142 epoch completed. current RMS loss 0.26338690519332886\n",
      "143 epoch completed. current RMS loss 0.2634749412536621\n",
      "144 epoch completed. current RMS loss 0.263528436422348\n",
      "145 epoch completed. current RMS loss 0.2635319232940674\n",
      "146 epoch completed. current RMS loss 0.2634921073913574\n",
      "147 epoch completed. current RMS loss 0.2634023427963257\n",
      "148 epoch completed. current RMS loss 0.26327642798423767\n",
      "149 epoch completed. current RMS loss 0.2631160020828247\n",
      "150 epoch completed. current RMS loss 0.2629314959049225\n",
      "151 epoch completed. current RMS loss 0.2627306580543518\n",
      "152 epoch completed. current RMS loss 0.2625216245651245\n",
      "153 epoch completed. current RMS loss 0.2623095214366913\n",
      "154 epoch completed. current RMS loss 0.2621067464351654\n",
      "155 epoch completed. current RMS loss 0.26192769408226013\n",
      "156 epoch completed. current RMS loss 0.2617771029472351\n",
      "157 epoch completed. current RMS loss 0.261659175157547\n",
      "158 epoch completed. current RMS loss 0.2615639865398407\n",
      "159 epoch completed. current RMS loss 0.26147520542144775\n",
      "160 epoch completed. current RMS loss 0.26139190793037415\n",
      "161 epoch completed. current RMS loss 0.26130834221839905\n",
      "162 epoch completed. current RMS loss 0.2612127661705017\n",
      "163 epoch completed. current RMS loss 0.2611010670661926\n",
      "164 epoch completed. current RMS loss 0.2609730064868927\n",
      "165 epoch completed. current RMS loss 0.2608256936073303\n",
      "166 epoch completed. current RMS loss 0.2606535255908966\n",
      "167 epoch completed. current RMS loss 0.2604607939720154\n",
      "168 epoch completed. current RMS loss 0.2602382004261017\n",
      "169 epoch completed. current RMS loss 0.25999024510383606\n",
      "170 epoch completed. current RMS loss 0.2597147822380066\n",
      "171 epoch completed. current RMS loss 0.2594147026538849\n",
      "172 epoch completed. current RMS loss 0.259102463722229\n",
      "173 epoch completed. current RMS loss 0.2587686777114868\n",
      "174 epoch completed. current RMS loss 0.2584249675273895\n",
      "175 epoch completed. current RMS loss 0.2580738067626953\n",
      "176 epoch completed. current RMS loss 0.25771039724349976\n",
      "177 epoch completed. current RMS loss 0.2573414742946625\n",
      "178 epoch completed. current RMS loss 0.25696566700935364\n",
      "179 epoch completed. current RMS loss 0.2565852403640747\n",
      "180 epoch completed. current RMS loss 0.2562057375907898\n",
      "181 epoch completed. current RMS loss 0.25582391023635864\n",
      "182 epoch completed. current RMS loss 0.255445659160614\n",
      "183 epoch completed. current RMS loss 0.255067378282547\n",
      "184 epoch completed. current RMS loss 0.2546936273574829\n",
      "185 epoch completed. current RMS loss 0.254324734210968\n",
      "186 epoch completed. current RMS loss 0.2539551556110382\n",
      "187 epoch completed. current RMS loss 0.253589391708374\n",
      "188 epoch completed. current RMS loss 0.25322169065475464\n",
      "189 epoch completed. current RMS loss 0.2528521418571472\n",
      "190 epoch completed. current RMS loss 0.25247687101364136\n",
      "191 epoch completed. current RMS loss 0.2520999312400818\n",
      "192 epoch completed. current RMS loss 0.2517094016075134\n",
      "193 epoch completed. current RMS loss 0.2513091266155243\n",
      "194 epoch completed. current RMS loss 0.25089505314826965\n",
      "195 epoch completed. current RMS loss 0.25047388672828674\n",
      "196 epoch completed. current RMS loss 0.2500373423099518\n",
      "197 epoch completed. current RMS loss 0.24960099160671234\n",
      "198 epoch completed. current RMS loss 0.24915194511413574\n",
      "199 epoch completed. current RMS loss 0.24870266020298004\n",
      "200 epoch completed. current RMS loss 0.2482534945011139\n",
      "201 epoch completed. current RMS loss 0.247815802693367\n",
      "202 epoch completed. current RMS loss 0.24738334119319916\n",
      "203 epoch completed. current RMS loss 0.24697253108024597\n",
      "204 epoch completed. current RMS loss 0.2465817779302597\n",
      "205 epoch completed. current RMS loss 0.24621279537677765\n",
      "206 epoch completed. current RMS loss 0.24586977064609528\n",
      "207 epoch completed. current RMS loss 0.24555328488349915\n",
      "208 epoch completed. current RMS loss 0.24526239931583405\n",
      "209 epoch completed. current RMS loss 0.24500149488449097\n",
      "210 epoch completed. current RMS loss 0.2447592318058014\n",
      "211 epoch completed. current RMS loss 0.2445397675037384\n",
      "212 epoch completed. current RMS loss 0.2443370223045349\n",
      "213 epoch completed. current RMS loss 0.24415315687656403\n",
      "214 epoch completed. current RMS loss 0.24397900700569153\n",
      "215 epoch completed. current RMS loss 0.2438252568244934\n",
      "216 epoch completed. current RMS loss 0.24368298053741455\n",
      "217 epoch completed. current RMS loss 0.2435598224401474\n",
      "218 epoch completed. current RMS loss 0.24345795810222626\n",
      "219 epoch completed. current RMS loss 0.2433682382106781\n",
      "220 epoch completed. current RMS loss 0.24330276250839233\n",
      "221 epoch completed. current RMS loss 0.24324938654899597\n",
      "222 epoch completed. current RMS loss 0.2432193160057068\n",
      "223 epoch completed. current RMS loss 0.24319979548454285\n",
      "224 epoch completed. current RMS loss 0.24319273233413696\n",
      "225 epoch completed. current RMS loss 0.24317863583564758\n",
      "226 epoch completed. current RMS loss 0.2431691586971283\n",
      "227 epoch completed. current RMS loss 0.24314726889133453\n",
      "228 epoch completed. current RMS loss 0.24311737716197968\n",
      "229 epoch completed. current RMS loss 0.2430715411901474\n",
      "230 epoch completed. current RMS loss 0.24301420152187347\n",
      "231 epoch completed. current RMS loss 0.2429397851228714\n",
      "232 epoch completed. current RMS loss 0.2428494393825531\n",
      "233 epoch completed. current RMS loss 0.242750883102417\n",
      "234 epoch completed. current RMS loss 0.2426394820213318\n",
      "235 epoch completed. current RMS loss 0.2425185889005661\n",
      "236 epoch completed. current RMS loss 0.2423897087574005\n",
      "237 epoch completed. current RMS loss 0.24225012958049774\n",
      "238 epoch completed. current RMS loss 0.24210523068904877\n",
      "239 epoch completed. current RMS loss 0.24195392429828644\n",
      "240 epoch completed. current RMS loss 0.24179114401340485\n",
      "241 epoch completed. current RMS loss 0.2416176050901413\n",
      "242 epoch completed. current RMS loss 0.24143947660923004\n",
      "243 epoch completed. current RMS loss 0.24125458300113678\n",
      "244 epoch completed. current RMS loss 0.24105891585350037\n",
      "245 epoch completed. current RMS loss 0.24085667729377747\n",
      "246 epoch completed. current RMS loss 0.24063941836357117\n",
      "247 epoch completed. current RMS loss 0.24042104184627533\n",
      "248 epoch completed. current RMS loss 0.24018462002277374\n",
      "249 epoch completed. current RMS loss 0.23993948101997375\n",
      "250 epoch completed. current RMS loss 0.2396819293498993\n",
      "251 epoch completed. current RMS loss 0.2394172102212906\n",
      "252 epoch completed. current RMS loss 0.23914943635463715\n",
      "253 epoch completed. current RMS loss 0.2388840615749359\n",
      "254 epoch completed. current RMS loss 0.23862262070178986\n",
      "255 epoch completed. current RMS loss 0.23836547136306763\n",
      "256 epoch completed. current RMS loss 0.23811088502407074\n",
      "257 epoch completed. current RMS loss 0.23786042630672455\n",
      "258 epoch completed. current RMS loss 0.23761098086833954\n",
      "259 epoch completed. current RMS loss 0.23736366629600525\n",
      "260 epoch completed. current RMS loss 0.23712006211280823\n",
      "261 epoch completed. current RMS loss 0.23687949776649475\n",
      "262 epoch completed. current RMS loss 0.23665349185466766\n",
      "263 epoch completed. current RMS loss 0.23643331229686737\n",
      "264 epoch completed. current RMS loss 0.23622296750545502\n",
      "265 epoch completed. current RMS loss 0.2360355108976364\n",
      "266 epoch completed. current RMS loss 0.23586329817771912\n",
      "267 epoch completed. current RMS loss 0.23570872843265533\n",
      "268 epoch completed. current RMS loss 0.2355683594942093\n",
      "269 epoch completed. current RMS loss 0.23541805148124695\n",
      "270 epoch completed. current RMS loss 0.2352626770734787\n",
      "271 epoch completed. current RMS loss 0.23509667813777924\n",
      "272 epoch completed. current RMS loss 0.23491831123828888\n",
      "273 epoch completed. current RMS loss 0.2347245216369629\n",
      "274 epoch completed. current RMS loss 0.23452138900756836\n",
      "275 epoch completed. current RMS loss 0.23429988324642181\n",
      "276 epoch completed. current RMS loss 0.2340761423110962\n",
      "277 epoch completed. current RMS loss 0.23383857309818268\n",
      "278 epoch completed. current RMS loss 0.23358482122421265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279 epoch completed. current RMS loss 0.2333282083272934\n",
      "280 epoch completed. current RMS loss 0.2330635040998459\n",
      "281 epoch completed. current RMS loss 0.23279541730880737\n",
      "282 epoch completed. current RMS loss 0.23251773416996002\n",
      "283 epoch completed. current RMS loss 0.23223187029361725\n",
      "284 epoch completed. current RMS loss 0.23194007575511932\n",
      "285 epoch completed. current RMS loss 0.23165152966976166\n",
      "286 epoch completed. current RMS loss 0.23135510087013245\n",
      "287 epoch completed. current RMS loss 0.23105226457118988\n",
      "288 epoch completed. current RMS loss 0.23075580596923828\n",
      "289 epoch completed. current RMS loss 0.23044466972351074\n",
      "290 epoch completed. current RMS loss 0.23012971878051758\n",
      "291 epoch completed. current RMS loss 0.22982074320316315\n",
      "292 epoch completed. current RMS loss 0.22949102520942688\n",
      "293 epoch completed. current RMS loss 0.22916792333126068\n",
      "294 epoch completed. current RMS loss 0.22883649170398712\n",
      "295 epoch completed. current RMS loss 0.22850261628627777\n",
      "296 epoch completed. current RMS loss 0.22817401587963104\n",
      "297 epoch completed. current RMS loss 0.22785264253616333\n",
      "298 epoch completed. current RMS loss 0.22753790020942688\n",
      "299 epoch completed. current RMS loss 0.22723901271820068\n",
      "300 epoch completed. current RMS loss 0.22695767879486084\n",
      "301 epoch completed. current RMS loss 0.2266995906829834\n",
      "302 epoch completed. current RMS loss 0.22645804286003113\n",
      "303 epoch completed. current RMS loss 0.2262483537197113\n",
      "304 epoch completed. current RMS loss 0.22606094181537628\n",
      "305 epoch completed. current RMS loss 0.2258947491645813\n",
      "306 epoch completed. current RMS loss 0.22575192153453827\n",
      "307 epoch completed. current RMS loss 0.22563223540782928\n",
      "308 epoch completed. current RMS loss 0.2255278378725052\n",
      "309 epoch completed. current RMS loss 0.22543366253376007\n",
      "310 epoch completed. current RMS loss 0.2253449261188507\n",
      "311 epoch completed. current RMS loss 0.2252567708492279\n",
      "312 epoch completed. current RMS loss 0.2251625955104828\n",
      "313 epoch completed. current RMS loss 0.22506779432296753\n",
      "314 epoch completed. current RMS loss 0.2249530851840973\n",
      "315 epoch completed. current RMS loss 0.22483564913272858\n",
      "316 epoch completed. current RMS loss 0.22470010817050934\n",
      "317 epoch completed. current RMS loss 0.22454886138439178\n",
      "318 epoch completed. current RMS loss 0.22438029944896698\n",
      "319 epoch completed. current RMS loss 0.2241860181093216\n",
      "320 epoch completed. current RMS loss 0.22397680580615997\n",
      "321 epoch completed. current RMS loss 0.2237483412027359\n",
      "322 epoch completed. current RMS loss 0.22349436581134796\n",
      "323 epoch completed. current RMS loss 0.22323185205459595\n",
      "324 epoch completed. current RMS loss 0.22295159101486206\n",
      "325 epoch completed. current RMS loss 0.22266510128974915\n",
      "326 epoch completed. current RMS loss 0.22236092388629913\n",
      "327 epoch completed. current RMS loss 0.22205114364624023\n",
      "328 epoch completed. current RMS loss 0.2217312753200531\n",
      "329 epoch completed. current RMS loss 0.2214144617319107\n",
      "330 epoch completed. current RMS loss 0.22110125422477722\n",
      "331 epoch completed. current RMS loss 0.22078488767147064\n",
      "332 epoch completed. current RMS loss 0.2204788476228714\n",
      "333 epoch completed. current RMS loss 0.22019526362419128\n",
      "334 epoch completed. current RMS loss 0.21991656720638275\n",
      "335 epoch completed. current RMS loss 0.2196560949087143\n",
      "336 epoch completed. current RMS loss 0.21939407289028168\n",
      "337 epoch completed. current RMS loss 0.2191423922777176\n",
      "338 epoch completed. current RMS loss 0.2189023792743683\n",
      "339 epoch completed. current RMS loss 0.21867012977600098\n",
      "340 epoch completed. current RMS loss 0.21845239400863647\n",
      "341 epoch completed. current RMS loss 0.21823358535766602\n",
      "342 epoch completed. current RMS loss 0.2180199772119522\n",
      "343 epoch completed. current RMS loss 0.21781808137893677\n",
      "344 epoch completed. current RMS loss 0.21762359142303467\n",
      "345 epoch completed. current RMS loss 0.21742424368858337\n",
      "346 epoch completed. current RMS loss 0.2172391563653946\n",
      "347 epoch completed. current RMS loss 0.217056542634964\n",
      "348 epoch completed. current RMS loss 0.21687883138656616\n",
      "349 epoch completed. current RMS loss 0.21671247482299805\n",
      "350 epoch completed. current RMS loss 0.21653854846954346\n",
      "351 epoch completed. current RMS loss 0.21638086438179016\n",
      "352 epoch completed. current RMS loss 0.21621929109096527\n",
      "353 epoch completed. current RMS loss 0.21607019007205963\n",
      "354 epoch completed. current RMS loss 0.2159167379140854\n",
      "355 epoch completed. current RMS loss 0.21576818823814392\n",
      "356 epoch completed. current RMS loss 0.21562623977661133\n",
      "357 epoch completed. current RMS loss 0.21547931432724\n",
      "358 epoch completed. current RMS loss 0.21534107625484467\n",
      "359 epoch completed. current RMS loss 0.21519848704338074\n",
      "360 epoch completed. current RMS loss 0.21505926549434662\n",
      "361 epoch completed. current RMS loss 0.2149176299571991\n",
      "362 epoch completed. current RMS loss 0.21477501094341278\n",
      "363 epoch completed. current RMS loss 0.21463990211486816\n",
      "364 epoch completed. current RMS loss 0.21450097858905792\n",
      "365 epoch completed. current RMS loss 0.21436679363250732\n",
      "366 epoch completed. current RMS loss 0.21422922611236572\n",
      "367 epoch completed. current RMS loss 0.2140820324420929\n",
      "368 epoch completed. current RMS loss 0.21395106613636017\n",
      "369 epoch completed. current RMS loss 0.21381758153438568\n",
      "370 epoch completed. current RMS loss 0.21368542313575745\n",
      "371 epoch completed. current RMS loss 0.21355581283569336\n",
      "372 epoch completed. current RMS loss 0.21343006193637848\n",
      "373 epoch completed. current RMS loss 0.2133163958787918\n",
      "374 epoch completed. current RMS loss 0.21321289241313934\n",
      "375 epoch completed. current RMS loss 0.21312479674816132\n",
      "376 epoch completed. current RMS loss 0.21305786073207855\n",
      "377 epoch completed. current RMS loss 0.21300767362117767\n",
      "378 epoch completed. current RMS loss 0.21296538412570953\n",
      "379 epoch completed. current RMS loss 0.21293367445468903\n",
      "380 epoch completed. current RMS loss 0.2129133939743042\n",
      "381 epoch completed. current RMS loss 0.21288664638996124\n",
      "382 epoch completed. current RMS loss 0.21287070214748383\n",
      "383 epoch completed. current RMS loss 0.21285131573677063\n",
      "384 epoch completed. current RMS loss 0.21282950043678284\n",
      "385 epoch completed. current RMS loss 0.2128087729215622\n",
      "386 epoch completed. current RMS loss 0.2127884328365326\n",
      "387 epoch completed. current RMS loss 0.21276351809501648\n",
      "388 epoch completed. current RMS loss 0.21274791657924652\n",
      "389 epoch completed. current RMS loss 0.21272295713424683\n",
      "390 epoch completed. current RMS loss 0.21271143853664398\n",
      "391 epoch completed. current RMS loss 0.21266984939575195\n",
      "392 epoch completed. current RMS loss 0.21264539659023285\n",
      "393 epoch completed. current RMS loss 0.21261218190193176\n",
      "394 epoch completed. current RMS loss 0.21257571876049042\n",
      "395 epoch completed. current RMS loss 0.2125382125377655\n",
      "396 epoch completed. current RMS loss 0.2124900221824646\n",
      "397 epoch completed. current RMS loss 0.21243461966514587\n",
      "398 epoch completed. current RMS loss 0.21238268911838531\n",
      "399 epoch completed. current RMS loss 0.21230725944042206\n",
      "400 epoch completed. current RMS loss 0.21222756803035736\n",
      "401 epoch completed. current RMS loss 0.21213245391845703\n",
      "402 epoch completed. current RMS loss 0.21202431619167328\n",
      "403 epoch completed. current RMS loss 0.21190331876277924\n",
      "404 epoch completed. current RMS loss 0.2117626816034317\n",
      "405 epoch completed. current RMS loss 0.2116047888994217\n",
      "406 epoch completed. current RMS loss 0.21142436563968658\n",
      "407 epoch completed. current RMS loss 0.21122969686985016\n",
      "408 epoch completed. current RMS loss 0.21101272106170654\n",
      "409 epoch completed. current RMS loss 0.21078050136566162\n",
      "410 epoch completed. current RMS loss 0.21054454147815704\n",
      "411 epoch completed. current RMS loss 0.2102816104888916\n",
      "412 epoch completed. current RMS loss 0.2100292295217514\n",
      "413 epoch completed. current RMS loss 0.20975321531295776\n",
      "414 epoch completed. current RMS loss 0.20947150886058807\n",
      "415 epoch completed. current RMS loss 0.209202378988266\n",
      "416 epoch completed. current RMS loss 0.20892833173274994\n",
      "417 epoch completed. current RMS loss 0.20865988731384277\n",
      "418 epoch completed. current RMS loss 0.20838557183742523\n",
      "419 epoch completed. current RMS loss 0.20812860131263733\n",
      "420 epoch completed. current RMS loss 0.20787173509597778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 epoch completed. current RMS loss 0.2076127678155899\n",
      "422 epoch completed. current RMS loss 0.20735858380794525\n",
      "423 epoch completed. current RMS loss 0.2071167230606079\n",
      "424 epoch completed. current RMS loss 0.20687778294086456\n",
      "425 epoch completed. current RMS loss 0.20664405822753906\n",
      "426 epoch completed. current RMS loss 0.20641128718852997\n",
      "427 epoch completed. current RMS loss 0.20617720484733582\n",
      "428 epoch completed. current RMS loss 0.2059541791677475\n",
      "429 epoch completed. current RMS loss 0.20571820437908173\n",
      "430 epoch completed. current RMS loss 0.20547887682914734\n",
      "431 epoch completed. current RMS loss 0.2052338421344757\n",
      "432 epoch completed. current RMS loss 0.20498254895210266\n",
      "433 epoch completed. current RMS loss 0.20472753047943115\n",
      "434 epoch completed. current RMS loss 0.20446759462356567\n",
      "435 epoch completed. current RMS loss 0.20419743657112122\n",
      "436 epoch completed. current RMS loss 0.2039305865764618\n",
      "437 epoch completed. current RMS loss 0.2036735713481903\n",
      "438 epoch completed. current RMS loss 0.20343126356601715\n",
      "439 epoch completed. current RMS loss 0.20318599045276642\n",
      "440 epoch completed. current RMS loss 0.20296336710453033\n",
      "441 epoch completed. current RMS loss 0.20274131000041962\n",
      "442 epoch completed. current RMS loss 0.20254723727703094\n",
      "443 epoch completed. current RMS loss 0.2023538053035736\n",
      "444 epoch completed. current RMS loss 0.20216521620750427\n",
      "445 epoch completed. current RMS loss 0.20200088620185852\n",
      "446 epoch completed. current RMS loss 0.20183783769607544\n",
      "447 epoch completed. current RMS loss 0.20168612897396088\n",
      "448 epoch completed. current RMS loss 0.20154446363449097\n",
      "449 epoch completed. current RMS loss 0.2014053910970688\n",
      "450 epoch completed. current RMS loss 0.20128104090690613\n",
      "451 epoch completed. current RMS loss 0.20116521418094635\n",
      "452 epoch completed. current RMS loss 0.2010573148727417\n",
      "453 epoch completed. current RMS loss 0.20095236599445343\n",
      "454 epoch completed. current RMS loss 0.20083992183208466\n",
      "455 epoch completed. current RMS loss 0.20075444877147675\n",
      "456 epoch completed. current RMS loss 0.20065052807331085\n",
      "457 epoch completed. current RMS loss 0.20056581497192383\n",
      "458 epoch completed. current RMS loss 0.20046722888946533\n",
      "459 epoch completed. current RMS loss 0.20038850605487823\n",
      "460 epoch completed. current RMS loss 0.2002948820590973\n",
      "461 epoch completed. current RMS loss 0.20021314918994904\n",
      "462 epoch completed. current RMS loss 0.20013076066970825\n",
      "463 epoch completed. current RMS loss 0.20005913078784943\n",
      "464 epoch completed. current RMS loss 0.1999785453081131\n",
      "465 epoch completed. current RMS loss 0.19991454482078552\n",
      "466 epoch completed. current RMS loss 0.1998489648103714\n",
      "467 epoch completed. current RMS loss 0.1997935026884079\n",
      "468 epoch completed. current RMS loss 0.19973799586296082\n",
      "469 epoch completed. current RMS loss 0.1996908336877823\n",
      "470 epoch completed. current RMS loss 0.1996554136276245\n",
      "471 epoch completed. current RMS loss 0.199614018201828\n",
      "472 epoch completed. current RMS loss 0.19957813620567322\n",
      "473 epoch completed. current RMS loss 0.19955109059810638\n",
      "474 epoch completed. current RMS loss 0.19953863322734833\n",
      "475 epoch completed. current RMS loss 0.19951915740966797\n",
      "476 epoch completed. current RMS loss 0.1995188593864441\n",
      "477 epoch completed. current RMS loss 0.19952166080474854\n",
      "478 epoch completed. current RMS loss 0.19953252375125885\n",
      "479 epoch completed. current RMS loss 0.19955597817897797\n",
      "480 epoch completed. current RMS loss 0.1995830535888672\n",
      "481 epoch completed. current RMS loss 0.19961272180080414\n",
      "482 epoch completed. current RMS loss 0.19964922964572906\n",
      "483 epoch completed. current RMS loss 0.19968867301940918\n",
      "484 epoch completed. current RMS loss 0.19972795248031616\n",
      "485 epoch completed. current RMS loss 0.19977602362632751\n",
      "486 epoch completed. current RMS loss 0.19983123242855072\n",
      "487 epoch completed. current RMS loss 0.19988524913787842\n",
      "488 epoch completed. current RMS loss 0.1999489963054657\n",
      "489 epoch completed. current RMS loss 0.20002390444278717\n",
      "490 epoch completed. current RMS loss 0.20008967816829681\n",
      "491 epoch completed. current RMS loss 0.20016202330589294\n",
      "492 epoch completed. current RMS loss 0.20024268329143524\n",
      "493 epoch completed. current RMS loss 0.20031310617923737\n",
      "494 epoch completed. current RMS loss 0.20039744675159454\n",
      "495 epoch completed. current RMS loss 0.20048506557941437\n",
      "496 epoch completed. current RMS loss 0.20055782794952393\n",
      "497 epoch completed. current RMS loss 0.20063647627830505\n",
      "498 epoch completed. current RMS loss 0.20072084665298462\n",
      "499 epoch completed. current RMS loss 0.20079782605171204\n",
      "500 epoch completed. current RMS loss 0.200871542096138\n",
      "--- 10185.51198887825 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#with open('./log.txt', 'w') as f:    \n",
    "    #sys.stdout = f\n",
    "print(\"Experiment taken on : \",time.strftime(\"%c\"))\n",
    "start_time = time.time()\n",
    "main()\n",
    "end_time = (time.time() - start_time)\n",
    "print(\"--- %s seconds ---\" % end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
