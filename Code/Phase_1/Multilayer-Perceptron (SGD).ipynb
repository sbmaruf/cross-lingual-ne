{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embeded_matrix(dataAddress='./../Data/eng.train',frq = 2,debug=0):\n",
    "    if( debug == 1 ):\n",
    "        print(\"---------inside create_embeded_matrix function---------\\n\")\n",
    "        \n",
    "    assert(debug==0 or debug ==1)\n",
    "    \n",
    "    with open(dataAddress,'r') as data:\n",
    "        line = data.readlines()\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"Total data : %d\"%(len(line)))\n",
    "    \n",
    "    isO   = {}\n",
    "    all_X = []\n",
    "    all_Y = []\n",
    "    cnt = 0 ; \n",
    "    for i in line:\n",
    "        temp = i.split()\n",
    "        l = len(temp)     \n",
    "        if( l != 4 ):\n",
    "            continue\n",
    "        if( temp[3] == \"O\" ):\n",
    "            cnt = cnt+1 ; \n",
    "        all_X.append(temp[0]) ;\n",
    "        if( temp[0] not in isO ):\n",
    "            isO[ temp[0] ]  = 0 \n",
    "        if( temp[3] != \"O\" ):\n",
    "            isO[ temp[0] ]  = 1 \n",
    "    \n",
    "    if( debug == 1):\n",
    "        print(\"Total Number of **Other** NE =\",cnt)\n",
    "        init_reduce = len(line)-len(all_X) \n",
    "        print(\"Initial reduction : %d\"%(init_reduce))\n",
    "        print(\"Current total : %d\"%len(all_X))\n",
    "        \n",
    "    \n",
    "    \n",
    "    frqCnt=dict()\n",
    "    for i in all_X :\n",
    "        if( i not in frqCnt ):\n",
    "            frqCnt[i] = 1\n",
    "        else: \n",
    "            frqCnt[i] = frqCnt[i]+1\n",
    "            \n",
    "    if( debug == 1):\n",
    "        print(\"Size of the dictionary(Unique Words) : %d\"%len(frqCnt))\n",
    "        p = set(all_X) ;\n",
    "        assert( len(p) == len(frqCnt) )\n",
    "    \n",
    "    \n",
    "    shrinked_X = []\n",
    "    for k,v in frqCnt.items():\n",
    "        if( (v >= frq) and (isO[k] == 1) ):\n",
    "            shrinked_X.append(k) \n",
    "    shrinked_X.append(\"*?*other\")\n",
    "    \n",
    "    assert(len(shrinked_X) == len(set(shrinked_X)))\n",
    "    \n",
    "    em_hash = dict()\n",
    "    idx = 0 \n",
    "    for i in shrinked_X:\n",
    "        if i not in em_hash:\n",
    "            em_hash[i] = idx\n",
    "        idx = idx + 1\n",
    "    \n",
    "    if( debug == 1):\n",
    "        print(\"Total item after shrinking by frequency-{0} : {1}\".format(frq, len(shrinked_X)))\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"\\n---------end of create_embeded_matrix function---------\")\n",
    "    \n",
    "    return (shrinked_X,em_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_map(debug=0):\n",
    "    \"\"\"\n",
    "        Category defination\n",
    "        O = others\n",
    "        I/B = 2\n",
    "        LOC/MISC/ORG/PER = 4\n",
    "        Total number of Combination = 1+2*4 = 9\n",
    "    \"\"\"\n",
    "    if( debug == 1 ):\n",
    "        print(\"---------inside output_map function---------\\n\")\n",
    "    assert(debug ==0 or debug == 1)\n",
    "    \n",
    "    IB=[\"I\",\"B\"]\n",
    "    ST=[\"LOC\",\"MISC\",\"ORG\",\"PER\"]\n",
    "    output_type = []\n",
    "    for i in IB:\n",
    "        for j in ST:\n",
    "            output_type.append(i+\"-\"+j)\n",
    "    output_type.append(\"O\")\n",
    "    \n",
    "    ret = dict()\n",
    "    idx = 0 \n",
    "    for i in output_type:\n",
    "        ret[i] = idx\n",
    "        if( debug == 1 ):\n",
    "             print(i+\"'s index number is %d\"%idx )\n",
    "        idx = idx+1\n",
    "    if( debug == 1 ):\n",
    "        print(\"\\n---------end of inside output_map function---------\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def parse_input(dataAddress='./../Data/eng.train',debug=0):\n",
    "    if( debug == 1 ):\n",
    "        print(\"---------inside parse function---------\\n\")\n",
    "        \n",
    "    assert(debug==0 or debug ==1)\n",
    "    \n",
    "    with open(dataAddress,'r') as data:\n",
    "        line = data.readlines()\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"Total data : %d\"%(len(line)))\n",
    "    \n",
    "\n",
    "    all_X = []\n",
    "    all_Y = []\n",
    "    for i in line:\n",
    "        temp = i.split()\n",
    "        l = len(temp) \n",
    "        if( l != 4 ):\n",
    "            continue\n",
    "        #idx = output_hash[temp[3]]\n",
    "        #temp_one_hot_Y = np.zeros(len(output_hash));\n",
    "        #temp_one_hot_Y[idx] = 1 ;\n",
    "        \n",
    "        all_X.append(temp[0]) ;\n",
    "        all_Y.append(temp[3]) ;\n",
    "    \n",
    " \n",
    "    if( debug == 1):\n",
    "        init_reduce = len(line)-len(all_X) \n",
    "        print(\"Initial reduction : %d\"%(init_reduce))\n",
    "        print(\"Current total : %d\"%len(all_X))\n",
    "        assert(len(all_X)==len(all_Y))\n",
    "    \n",
    "    if( debug == 1 ):\n",
    "        print(\"\\n---------end of parse function---------\")\n",
    "    return all_X,all_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# information from datasets\n",
    "\n",
    "## eng.testa\n",
    "-----------\n",
    "+ I-LOC\n",
    "+ B-MISC\n",
    "+ O\n",
    "+ I-MISC\n",
    "+ I-PER\n",
    "+ I-ORG\n",
    "\n",
    "## eng.testb\n",
    "+ I-LOC\n",
    "+ B-MISC\n",
    "+ B-LOC\n",
    "+ O\n",
    "+ I-MISC\n",
    "+ I-PER\n",
    "+ I-ORG\n",
    "\n",
    "## eng.train\n",
    "+ I-LOC\n",
    "+ I-MISC\n",
    "+ I-PER\n",
    "+ I-ORG\n",
    "+ B-LOC\n",
    "+ B-MISC\n",
    "+ B-ORG\n",
    "+ O\n",
    "\n",
    "## temporary archived code\n",
    "\"\"\"\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RND_SEED = 100\n",
    "tf.set_random_seed(RND_SEED)\n",
    "\n",
    "\n",
    "    Embeded Matrix\n",
    "    EM = create_embeded_matrix(debug=1)\n",
    "    EM_X = EM[0] \n",
    "    EM_hash = EM[1]\n",
    "    take all the inputs from the eng.train. \n",
    "    output is represented in one hot encoding format\n",
    "    train_X, train_y = parse_input(debug=1)\n",
    "    number_of_softmax_layer = len(output_map())\n",
    "    \n",
    "\n",
    "    #design model\n",
    "    size_of_embeded_row = 100\n",
    "    for i in range(total_no_of_word):\n",
    "        W[i] = tf.Variable(tf.zeros([size_of_embeded_row = 100,number_of_softmax_layer]))\n",
    "        x[i] = tf.placeholder(tf.float32, [1, 100])\n",
    "        y += W[i]*x[i] ;\n",
    "\n",
    "    vocabulary_size = len(EM_X)\n",
    "    embedding_size = 100 \n",
    "    word_embeddings = tf.get_variable(\"word_embeddings\",[vocabulary_size, embedding_size])\n",
    "    \n",
    "    batch_size = len(EM_X)\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 9])\n",
    "    \n",
    "    embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, train_inputs)\n",
    "    print(embedded_word_ids.shape)\n",
    "    \n",
    "    nce_weights = tf.Variable(\n",
    "                      tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    num_sampled = batch_size;\n",
    "    init_loss = tf.reduce_mean(\n",
    "                 tf.nn.nce_loss(weights=nce_weights,\n",
    "                 biases=nce_biases,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embedded_word_ids,\n",
    "                 num_sampled=9,\n",
    "                 num_classes=vocabulary_size))\n",
    "    loss = tf.nn.softmax(init_loss)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.5).minimize(loss)\n",
    "    \n",
    "    feed_dict = {train_inputs:range(len(train_X)) ,def main(): train_labels: train_Y}\n",
    "    _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_input(batch_no,train_X,train_Y,batch_size=100):\n",
    "    row_st = batch_no*batch_size\n",
    "    if( row_st > len(train_X) ):\n",
    "        return [],[]\n",
    "    return train_X[row_st:min(row_st+batch_size,len(train_X)+1)],train_Y[row_st:min(row_st+batch_size,len(train_Y)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(vector,one_hot_hash):\n",
    "    retVec = []\n",
    "    for idx,val in enumerate(vector):\n",
    "        tmp_one_hot = np.zeros(len(one_hot_hash))\n",
    "        if val in one_hot_hash:\n",
    "            tmp_one_hot[ one_hot_hash[val] ] = 1 \n",
    "        else:\n",
    "            tmp_one_hot[ -1 ] = 1 \n",
    "        retVec.append(tmp_one_hot)\n",
    "    return retVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# With SGD/RMSProp/ADAM Training algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "RND_SEED = 100\n",
    "tf.set_random_seed(RND_SEED)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "        Hyper parameter list\n",
    "        \n",
    "        + EMBEDDING_DIM\n",
    "               \n",
    "    \"\"\"\n",
    "    \n",
    "    #Embeded Matrix\n",
    "    EM = create_embeded_matrix(debug=1)\n",
    "    EM_X = EM[0] \n",
    "    EM_hash = EM[1]\n",
    "    #print(EM_hash['EU'])\n",
    "    \n",
    "    #take all the inputs from the eng.train. \n",
    "    #output is represented in one hot encoding format\n",
    "    train_X, train_Y = parse_input(debug=1)\n",
    "    \n",
    "    train_X = np.asarray(train_X)\n",
    "    train_Y = np.asarray(train_Y)\n",
    "    \n",
    "    #print(train_X[1:10])\n",
    "    #print(train_Y[1:10])\n",
    "    \n",
    "    Y_hash = output_map()\n",
    "    output_size = len(Y_hash)\n",
    "    \n",
    "    #print(train_X.shape,train_Y.shape)\n",
    "\n",
    "    vocabulary_size = len(EM_X)\n",
    "    x = tf.placeholder(tf.float32,shape=[None,vocabulary_size])\n",
    "    y_goal = tf.placeholder(tf.float32, shape=[None,output_size])\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([vocabulary_size,EMBEDDING_DIM]))\n",
    "    b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "    hidden_representation = tf.add(tf.matmul(x,W1),b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM,output_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([output_size]))\n",
    "    prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_representation,W2),b2))\n",
    "    \n",
    "    cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_goal*tf.log(prediction),reduction_indices=[1]))\n",
    "    \n",
    "    train_stepSGD = tf.train.GradientDescentOptimizer(.00001).minimize(cross_entropy_loss)\n",
    "    #train_stepRMS = tf.train.RMSPropOptimizer(.0001).minimize(cross_entropy_loss)\n",
    "    #train_stepADAM = tf.train.AdamOptimizer(learning_rate=.0001).minimize(cross_entropy_loss)\n",
    "\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    epoch = 500\n",
    "    \n",
    "    \n",
    "    print(\"\\nEntering training Session\")\n",
    "    print(\"-------------------------\")\n",
    "    for itr in range(epoch):\n",
    "        batch_no = 0  \n",
    "        a = -1\n",
    "        b = -1 \n",
    "        while( 1 ):\n",
    "            curr_X, curr_Y = get_batch_input(batch_no,train_X,train_Y,batch_size=100)\n",
    "            if( len(curr_X) == 0 ):\n",
    "                break\n",
    "            curr_X = one_hot_embedding(curr_X,EM_hash)\n",
    "            curr_Y = one_hot_embedding(curr_Y,Y_hash)\n",
    "            curr_X = np.asarray(curr_X)\n",
    "            curr_Y = np.asarray(curr_Y)\n",
    "            a, b = sess.run([train_stepSGD, cross_entropy_loss],feed_dict={x:curr_X,y_goal:curr_Y})\n",
    "            #a, b = sess.run([train_stepRMS, cross_entropy_loss],feed_dict={x:curr_X,y_goal:curr_Y})\n",
    "            #a, b = sess.run([train_stepADAM, cross_entropy_loss],feed_dict={x:curr_X,y_goal:curr_Y})\n",
    "            batch_no = batch_no + 1 \n",
    "            #print(b)\n",
    "        print(\"{0} epoch completed. current SGD loss {1}\".format(itr+1,b))\n",
    "        #print(\"{0} epoch completed. current RMS loss {1}\".format(itr+1,b))\n",
    "        #print(\"{0} epoch completed. current ADAM loss {1}\".format(itr+1,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment taken on :  Tue Oct 17 17:33:43 2017\n",
      "---------inside create_embeded_matrix function---------\n",
      "\n",
      "Total data : 219553\n",
      "Total Number of **Other** NE = 170524\n",
      "Initial reduction : 14986\n",
      "Current total : 204567\n",
      "Size of the dictionary(Unique Words) : 23624\n",
      "Total item after shrinking by frequency-2 : 4575\n",
      "\n",
      "---------end of create_embeded_matrix function---------\n",
      "---------inside parse function---------\n",
      "\n",
      "Total data : 219553\n",
      "Initial reduction : 14986\n",
      "Current total : 204567\n",
      "\n",
      "---------end of parse function---------\n",
      "\n",
      "Entering training Session\n",
      "-------------------------\n",
      "1 epoch completed. current SGD loss 15.543291091918945\n",
      "2 epoch completed. current SGD loss 13.234321594238281\n",
      "3 epoch completed. current SGD loss 11.180703163146973\n",
      "4 epoch completed. current SGD loss 9.38178825378418\n",
      "5 epoch completed. current SGD loss 7.868096828460693\n",
      "6 epoch completed. current SGD loss 7.293787002563477\n",
      "7 epoch completed. current SGD loss 7.216007709503174\n",
      "8 epoch completed. current SGD loss 7.171381950378418\n",
      "9 epoch completed. current SGD loss 7.126232624053955\n",
      "10 epoch completed. current SGD loss 7.079232692718506\n",
      "11 epoch completed. current SGD loss 7.031262397766113\n",
      "12 epoch completed. current SGD loss 6.983097076416016\n",
      "13 epoch completed. current SGD loss 6.935251235961914\n",
      "14 epoch completed. current SGD loss 6.888030529022217\n",
      "15 epoch completed. current SGD loss 6.841614246368408\n",
      "16 epoch completed. current SGD loss 6.79610013961792\n",
      "17 epoch completed. current SGD loss 6.751525402069092\n",
      "18 epoch completed. current SGD loss 6.707901954650879\n",
      "19 epoch completed. current SGD loss 6.665215015411377\n",
      "20 epoch completed. current SGD loss 6.62344217300415\n",
      "21 epoch completed. current SGD loss 6.582563400268555\n",
      "22 epoch completed. current SGD loss 6.542544364929199\n",
      "23 epoch completed. current SGD loss 6.503359794616699\n",
      "24 epoch completed. current SGD loss 6.464978218078613\n",
      "25 epoch completed. current SGD loss 6.4273810386657715\n",
      "26 epoch completed. current SGD loss 6.390550136566162\n",
      "27 epoch completed. current SGD loss 6.354465961456299\n",
      "28 epoch completed. current SGD loss 6.3191070556640625\n",
      "29 epoch completed. current SGD loss 6.2844696044921875\n",
      "30 epoch completed. current SGD loss 6.2505388259887695\n",
      "31 epoch completed. current SGD loss 6.217310428619385\n",
      "32 epoch completed. current SGD loss 6.184771537780762\n",
      "33 epoch completed. current SGD loss 6.152911186218262\n",
      "34 epoch completed. current SGD loss 6.121708869934082\n",
      "35 epoch completed. current SGD loss 6.091147422790527\n",
      "36 epoch completed. current SGD loss 6.061197757720947\n",
      "37 epoch completed. current SGD loss 6.031834125518799\n",
      "38 epoch completed. current SGD loss 6.003031253814697\n",
      "39 epoch completed. current SGD loss 5.974763870239258\n",
      "40 epoch completed. current SGD loss 5.947009563446045\n",
      "41 epoch completed. current SGD loss 5.91974401473999\n",
      "42 epoch completed. current SGD loss 5.8929524421691895\n",
      "43 epoch completed. current SGD loss 5.866625785827637\n",
      "44 epoch completed. current SGD loss 5.840742111206055\n",
      "45 epoch completed. current SGD loss 5.8152923583984375\n",
      "46 epoch completed. current SGD loss 5.790268898010254\n",
      "47 epoch completed. current SGD loss 5.76566219329834\n",
      "48 epoch completed. current SGD loss 5.7414679527282715\n",
      "49 epoch completed. current SGD loss 5.717679500579834\n",
      "50 epoch completed. current SGD loss 5.69428825378418\n",
      "51 epoch completed. current SGD loss 5.671285629272461\n",
      "52 epoch completed. current SGD loss 5.648665428161621\n",
      "53 epoch completed. current SGD loss 5.6264190673828125\n",
      "54 epoch completed. current SGD loss 5.604543209075928\n",
      "55 epoch completed. current SGD loss 5.5830302238464355\n",
      "56 epoch completed. current SGD loss 5.5618720054626465\n",
      "57 epoch completed. current SGD loss 5.541060924530029\n",
      "58 epoch completed. current SGD loss 5.520590305328369\n",
      "59 epoch completed. current SGD loss 5.500452518463135\n",
      "60 epoch completed. current SGD loss 5.480641841888428\n",
      "61 epoch completed. current SGD loss 5.461150169372559\n",
      "62 epoch completed. current SGD loss 5.441971302032471\n",
      "63 epoch completed. current SGD loss 5.423097610473633\n",
      "64 epoch completed. current SGD loss 5.404523849487305\n",
      "65 epoch completed. current SGD loss 5.38624382019043\n",
      "66 epoch completed. current SGD loss 5.3682541847229\n",
      "67 epoch completed. current SGD loss 5.350550174713135\n",
      "68 epoch completed. current SGD loss 5.333128452301025\n",
      "69 epoch completed. current SGD loss 5.315983772277832\n",
      "70 epoch completed. current SGD loss 5.2991132736206055\n",
      "71 epoch completed. current SGD loss 5.282515525817871\n",
      "72 epoch completed. current SGD loss 5.266191005706787\n",
      "73 epoch completed. current SGD loss 5.250137805938721\n",
      "74 epoch completed. current SGD loss 5.234355926513672\n",
      "75 epoch completed. current SGD loss 5.2188496589660645\n",
      "76 epoch completed. current SGD loss 5.203617572784424\n",
      "77 epoch completed. current SGD loss 5.188663959503174\n",
      "78 epoch completed. current SGD loss 5.173992156982422\n",
      "79 epoch completed. current SGD loss 5.159606456756592\n",
      "80 epoch completed. current SGD loss 5.145510673522949\n",
      "81 epoch completed. current SGD loss 5.131702899932861\n",
      "82 epoch completed. current SGD loss 5.118188858032227\n",
      "83 epoch completed. current SGD loss 5.104970455169678\n",
      "84 epoch completed. current SGD loss 5.092049598693848\n",
      "85 epoch completed. current SGD loss 5.079422950744629\n",
      "86 epoch completed. current SGD loss 5.067091464996338\n",
      "87 epoch completed. current SGD loss 5.055049419403076\n",
      "88 epoch completed. current SGD loss 5.043302536010742\n",
      "89 epoch completed. current SGD loss 5.031842231750488\n",
      "90 epoch completed. current SGD loss 5.020662307739258\n",
      "91 epoch completed. current SGD loss 5.009762763977051\n",
      "92 epoch completed. current SGD loss 4.999131202697754\n",
      "93 epoch completed. current SGD loss 4.988766670227051\n",
      "94 epoch completed. current SGD loss 4.978660583496094\n",
      "95 epoch completed. current SGD loss 4.968810081481934\n",
      "96 epoch completed. current SGD loss 4.9592084884643555\n",
      "97 epoch completed. current SGD loss 4.9498515129089355\n",
      "98 epoch completed. current SGD loss 4.940732479095459\n",
      "99 epoch completed. current SGD loss 4.931848526000977\n",
      "100 epoch completed. current SGD loss 4.92318868637085\n",
      "101 epoch completed. current SGD loss 4.914748668670654\n",
      "102 epoch completed. current SGD loss 4.906520366668701\n",
      "103 epoch completed. current SGD loss 4.898500442504883\n",
      "104 epoch completed. current SGD loss 4.890678405761719\n",
      "105 epoch completed. current SGD loss 4.883048057556152\n",
      "106 epoch completed. current SGD loss 4.875604629516602\n",
      "107 epoch completed. current SGD loss 4.868340492248535\n",
      "108 epoch completed. current SGD loss 4.861248970031738\n",
      "109 epoch completed. current SGD loss 4.854320526123047\n",
      "110 epoch completed. current SGD loss 4.8475494384765625\n",
      "111 epoch completed. current SGD loss 4.8409247398376465\n",
      "112 epoch completed. current SGD loss 4.8344407081604\n",
      "113 epoch completed. current SGD loss 4.828089714050293\n",
      "114 epoch completed. current SGD loss 4.821865558624268\n",
      "115 epoch completed. current SGD loss 4.815762042999268\n",
      "116 epoch completed. current SGD loss 4.809774398803711\n",
      "117 epoch completed. current SGD loss 4.803893089294434\n",
      "118 epoch completed. current SGD loss 4.7981133460998535\n",
      "119 epoch completed. current SGD loss 4.792431831359863\n",
      "120 epoch completed. current SGD loss 4.786839962005615\n",
      "121 epoch completed. current SGD loss 4.781337261199951\n",
      "122 epoch completed. current SGD loss 4.775917053222656\n",
      "123 epoch completed. current SGD loss 4.770574569702148\n",
      "124 epoch completed. current SGD loss 4.76530647277832\n",
      "125 epoch completed. current SGD loss 4.760107040405273\n",
      "126 epoch completed. current SGD loss 4.754971504211426\n",
      "127 epoch completed. current SGD loss 4.7499003410339355\n",
      "128 epoch completed. current SGD loss 4.744889259338379\n",
      "129 epoch completed. current SGD loss 4.739934921264648\n",
      "130 epoch completed. current SGD loss 4.735034942626953\n",
      "131 epoch completed. current SGD loss 4.73018741607666\n",
      "132 epoch completed. current SGD loss 4.72538948059082\n",
      "133 epoch completed. current SGD loss 4.720641613006592\n",
      "134 epoch completed. current SGD loss 4.715938568115234\n",
      "135 epoch completed. current SGD loss 4.711280345916748\n",
      "136 epoch completed. current SGD loss 4.706664085388184\n",
      "137 epoch completed. current SGD loss 4.702091217041016\n",
      "138 epoch completed. current SGD loss 4.697556495666504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 epoch completed. current SGD loss 4.693061828613281\n",
      "140 epoch completed. current SGD loss 4.688607215881348\n",
      "141 epoch completed. current SGD loss 4.6841888427734375\n",
      "142 epoch completed. current SGD loss 4.679806709289551\n",
      "143 epoch completed. current SGD loss 4.675460338592529\n",
      "144 epoch completed. current SGD loss 4.671146392822266\n",
      "145 epoch completed. current SGD loss 4.666865825653076\n",
      "146 epoch completed. current SGD loss 4.662616729736328\n",
      "147 epoch completed. current SGD loss 4.658398151397705\n",
      "148 epoch completed. current SGD loss 4.654210567474365\n",
      "149 epoch completed. current SGD loss 4.650050640106201\n",
      "150 epoch completed. current SGD loss 4.6459221839904785\n",
      "151 epoch completed. current SGD loss 4.641819477081299\n",
      "152 epoch completed. current SGD loss 4.637744426727295\n",
      "153 epoch completed. current SGD loss 4.633694648742676\n",
      "154 epoch completed. current SGD loss 4.629669189453125\n",
      "155 epoch completed. current SGD loss 4.625670433044434\n",
      "156 epoch completed. current SGD loss 4.621697425842285\n",
      "157 epoch completed. current SGD loss 4.617747783660889\n",
      "158 epoch completed. current SGD loss 4.6138224601745605\n",
      "159 epoch completed. current SGD loss 4.609921932220459\n",
      "160 epoch completed. current SGD loss 4.606044292449951\n",
      "161 epoch completed. current SGD loss 4.602187156677246\n",
      "162 epoch completed. current SGD loss 4.598351955413818\n",
      "163 epoch completed. current SGD loss 4.59453821182251\n",
      "164 epoch completed. current SGD loss 4.5907440185546875\n",
      "165 epoch completed. current SGD loss 4.58696985244751\n",
      "166 epoch completed. current SGD loss 4.583215713500977\n",
      "167 epoch completed. current SGD loss 4.579480171203613\n",
      "168 epoch completed. current SGD loss 4.575764179229736\n",
      "169 epoch completed. current SGD loss 4.57206916809082\n",
      "170 epoch completed. current SGD loss 4.568392276763916\n",
      "171 epoch completed. current SGD loss 4.564733505249023\n",
      "172 epoch completed. current SGD loss 4.56109094619751\n",
      "173 epoch completed. current SGD loss 4.55746603012085\n",
      "174 epoch completed. current SGD loss 4.553858757019043\n",
      "175 epoch completed. current SGD loss 4.550268173217773\n",
      "176 epoch completed. current SGD loss 4.546693325042725\n",
      "177 epoch completed. current SGD loss 4.543135166168213\n",
      "178 epoch completed. current SGD loss 4.539591312408447\n",
      "179 epoch completed. current SGD loss 4.536062717437744\n",
      "180 epoch completed. current SGD loss 4.532547473907471\n",
      "181 epoch completed. current SGD loss 4.52904748916626\n",
      "182 epoch completed. current SGD loss 4.525562286376953\n",
      "183 epoch completed. current SGD loss 4.52208948135376\n",
      "184 epoch completed. current SGD loss 4.518631458282471\n",
      "185 epoch completed. current SGD loss 4.515186309814453\n",
      "186 epoch completed. current SGD loss 4.511755466461182\n",
      "187 epoch completed. current SGD loss 4.50833797454834\n",
      "188 epoch completed. current SGD loss 4.504934787750244\n",
      "189 epoch completed. current SGD loss 4.50154447555542\n",
      "190 epoch completed. current SGD loss 4.498167037963867\n",
      "191 epoch completed. current SGD loss 4.4948039054870605\n",
      "192 epoch completed. current SGD loss 4.491452217102051\n",
      "193 epoch completed. current SGD loss 4.4881110191345215\n",
      "194 epoch completed. current SGD loss 4.484781742095947\n",
      "195 epoch completed. current SGD loss 4.481461524963379\n",
      "196 epoch completed. current SGD loss 4.47815465927124\n",
      "197 epoch completed. current SGD loss 4.474859237670898\n",
      "198 epoch completed. current SGD loss 4.471573352813721\n",
      "199 epoch completed. current SGD loss 4.46829891204834\n",
      "200 epoch completed. current SGD loss 4.465036392211914\n",
      "201 epoch completed. current SGD loss 4.4617838859558105\n",
      "202 epoch completed. current SGD loss 4.458540916442871\n",
      "203 epoch completed. current SGD loss 4.455309867858887\n",
      "204 epoch completed. current SGD loss 4.452088832855225\n",
      "205 epoch completed. current SGD loss 4.448876857757568\n",
      "206 epoch completed. current SGD loss 4.445673942565918\n",
      "207 epoch completed. current SGD loss 4.442480564117432\n",
      "208 epoch completed. current SGD loss 4.439296722412109\n",
      "209 epoch completed. current SGD loss 4.436121940612793\n",
      "210 epoch completed. current SGD loss 4.432958602905273\n",
      "211 epoch completed. current SGD loss 4.429802894592285\n",
      "212 epoch completed. current SGD loss 4.426656246185303\n",
      "213 epoch completed. current SGD loss 4.42351770401001\n",
      "214 epoch completed. current SGD loss 4.420389652252197\n",
      "215 epoch completed. current SGD loss 4.4172682762146\n",
      "216 epoch completed. current SGD loss 4.414155960083008\n",
      "217 epoch completed. current SGD loss 4.411050796508789\n",
      "218 epoch completed. current SGD loss 4.40795373916626\n",
      "219 epoch completed. current SGD loss 4.4048638343811035\n",
      "220 epoch completed. current SGD loss 4.40178108215332\n",
      "221 epoch completed. current SGD loss 4.398706912994385\n",
      "222 epoch completed. current SGD loss 4.395641326904297\n",
      "223 epoch completed. current SGD loss 4.39258337020874\n",
      "224 epoch completed. current SGD loss 4.389532089233398\n",
      "225 epoch completed. current SGD loss 4.3864874839782715\n",
      "226 epoch completed. current SGD loss 4.383449077606201\n",
      "227 epoch completed. current SGD loss 4.38041877746582\n",
      "228 epoch completed. current SGD loss 4.377394199371338\n",
      "229 epoch completed. current SGD loss 4.374375820159912\n",
      "230 epoch completed. current SGD loss 4.371364593505859\n",
      "231 epoch completed. current SGD loss 4.36836051940918\n",
      "232 epoch completed. current SGD loss 4.365362167358398\n",
      "233 epoch completed. current SGD loss 4.362370014190674\n",
      "234 epoch completed. current SGD loss 4.359383583068848\n",
      "235 epoch completed. current SGD loss 4.356404781341553\n",
      "236 epoch completed. current SGD loss 4.353431701660156\n",
      "237 epoch completed. current SGD loss 4.350465774536133\n",
      "238 epoch completed. current SGD loss 4.347506046295166\n",
      "239 epoch completed. current SGD loss 4.344552040100098\n",
      "240 epoch completed. current SGD loss 4.341604709625244\n",
      "241 epoch completed. current SGD loss 4.338663578033447\n",
      "242 epoch completed. current SGD loss 4.335728168487549\n",
      "243 epoch completed. current SGD loss 4.332798957824707\n",
      "244 epoch completed. current SGD loss 4.329875946044922\n",
      "245 epoch completed. current SGD loss 4.326959133148193\n",
      "246 epoch completed. current SGD loss 4.324048042297363\n",
      "247 epoch completed. current SGD loss 4.321141242980957\n",
      "248 epoch completed. current SGD loss 4.318241596221924\n",
      "249 epoch completed. current SGD loss 4.315347671508789\n",
      "250 epoch completed. current SGD loss 4.312458038330078\n",
      "251 epoch completed. current SGD loss 4.30957555770874\n",
      "252 epoch completed. current SGD loss 4.306699275970459\n",
      "253 epoch completed. current SGD loss 4.303828716278076\n",
      "254 epoch completed. current SGD loss 4.30096435546875\n",
      "255 epoch completed. current SGD loss 4.298104763031006\n",
      "256 epoch completed. current SGD loss 4.295252323150635\n",
      "257 epoch completed. current SGD loss 4.292405128479004\n",
      "258 epoch completed. current SGD loss 4.289562702178955\n",
      "259 epoch completed. current SGD loss 4.286725044250488\n",
      "260 epoch completed. current SGD loss 4.283892631530762\n",
      "261 epoch completed. current SGD loss 4.281064510345459\n",
      "262 epoch completed. current SGD loss 4.278242588043213\n",
      "263 epoch completed. current SGD loss 4.275424957275391\n",
      "264 epoch completed. current SGD loss 4.27261209487915\n",
      "265 epoch completed. current SGD loss 4.269804954528809\n",
      "266 epoch completed. current SGD loss 4.267001628875732\n",
      "267 epoch completed. current SGD loss 4.264203071594238\n",
      "268 epoch completed. current SGD loss 4.261409282684326\n",
      "269 epoch completed. current SGD loss 4.25861930847168\n",
      "270 epoch completed. current SGD loss 4.255833148956299\n",
      "271 epoch completed. current SGD loss 4.2530517578125\n",
      "272 epoch completed. current SGD loss 4.250274658203125\n",
      "273 epoch completed. current SGD loss 4.24750280380249\n",
      "274 epoch completed. current SGD loss 4.244734764099121\n",
      "275 epoch completed. current SGD loss 4.241971015930176\n",
      "276 epoch completed. current SGD loss 4.2392120361328125\n",
      "277 epoch completed. current SGD loss 4.236456394195557\n",
      "278 epoch completed. current SGD loss 4.233705520629883\n",
      "279 epoch completed. current SGD loss 4.230958938598633\n",
      "280 epoch completed. current SGD loss 4.228217124938965\n",
      "281 epoch completed. current SGD loss 4.225479602813721\n",
      "282 epoch completed. current SGD loss 4.222746849060059\n",
      "283 epoch completed. current SGD loss 4.220016956329346\n",
      "284 epoch completed. current SGD loss 4.217292785644531\n",
      "285 epoch completed. current SGD loss 4.214572429656982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 epoch completed. current SGD loss 4.211857318878174\n",
      "287 epoch completed. current SGD loss 4.209146022796631\n",
      "288 epoch completed. current SGD loss 4.2064385414123535\n",
      "289 epoch completed. current SGD loss 4.2037353515625\n",
      "290 epoch completed. current SGD loss 4.201035499572754\n",
      "291 epoch completed. current SGD loss 4.198339939117432\n",
      "292 epoch completed. current SGD loss 4.195648670196533\n",
      "293 epoch completed. current SGD loss 4.1929612159729\n",
      "294 epoch completed. current SGD loss 4.190277576446533\n",
      "295 epoch completed. current SGD loss 4.18759822845459\n",
      "296 epoch completed. current SGD loss 4.18492317199707\n",
      "297 epoch completed. current SGD loss 4.182251453399658\n",
      "298 epoch completed. current SGD loss 4.179583549499512\n",
      "299 epoch completed. current SGD loss 4.176919937133789\n",
      "300 epoch completed. current SGD loss 4.174261569976807\n",
      "301 epoch completed. current SGD loss 4.171605110168457\n",
      "302 epoch completed. current SGD loss 4.168952941894531\n",
      "303 epoch completed. current SGD loss 4.166304111480713\n",
      "304 epoch completed. current SGD loss 4.163658142089844\n",
      "305 epoch completed. current SGD loss 4.16101598739624\n",
      "306 epoch completed. current SGD loss 4.158376216888428\n",
      "307 epoch completed. current SGD loss 4.155740737915039\n",
      "308 epoch completed. current SGD loss 4.153109073638916\n",
      "309 epoch completed. current SGD loss 4.150481224060059\n",
      "310 epoch completed. current SGD loss 4.14785623550415\n",
      "311 epoch completed. current SGD loss 4.14523458480835\n",
      "312 epoch completed. current SGD loss 4.142618179321289\n",
      "313 epoch completed. current SGD loss 4.140006065368652\n",
      "314 epoch completed. current SGD loss 4.137396335601807\n",
      "315 epoch completed. current SGD loss 4.134790420532227\n",
      "316 epoch completed. current SGD loss 4.132187843322754\n",
      "317 epoch completed. current SGD loss 4.12959098815918\n",
      "318 epoch completed. current SGD loss 4.1269965171813965\n",
      "319 epoch completed. current SGD loss 4.124405860900879\n",
      "320 epoch completed. current SGD loss 4.121819972991943\n",
      "321 epoch completed. current SGD loss 4.119237422943115\n",
      "322 epoch completed. current SGD loss 4.1166582107543945\n",
      "323 epoch completed. current SGD loss 4.114083290100098\n",
      "324 epoch completed. current SGD loss 4.111509799957275\n",
      "325 epoch completed. current SGD loss 4.108941078186035\n",
      "326 epoch completed. current SGD loss 4.1063761711120605\n",
      "327 epoch completed. current SGD loss 4.103814125061035\n",
      "328 epoch completed. current SGD loss 4.101255893707275\n",
      "329 epoch completed. current SGD loss 4.098701477050781\n",
      "330 epoch completed. current SGD loss 4.096149921417236\n",
      "331 epoch completed. current SGD loss 4.093602657318115\n",
      "332 epoch completed. current SGD loss 4.091057777404785\n",
      "333 epoch completed. current SGD loss 4.088517189025879\n",
      "334 epoch completed. current SGD loss 4.085978984832764\n",
      "335 epoch completed. current SGD loss 4.083444595336914\n",
      "336 epoch completed. current SGD loss 4.080913543701172\n",
      "337 epoch completed. current SGD loss 4.078386306762695\n",
      "338 epoch completed. current SGD loss 4.075861930847168\n",
      "339 epoch completed. current SGD loss 4.07334041595459\n",
      "340 epoch completed. current SGD loss 4.070824146270752\n",
      "341 epoch completed. current SGD loss 4.0683088302612305\n",
      "342 epoch completed. current SGD loss 4.065798282623291\n",
      "343 epoch completed. current SGD loss 4.063291072845459\n",
      "344 epoch completed. current SGD loss 4.060788154602051\n",
      "345 epoch completed. current SGD loss 4.058287620544434\n",
      "346 epoch completed. current SGD loss 4.055789947509766\n",
      "347 epoch completed. current SGD loss 4.053296089172363\n",
      "348 epoch completed. current SGD loss 4.050805568695068\n",
      "349 epoch completed. current SGD loss 4.048317909240723\n",
      "350 epoch completed. current SGD loss 4.045834064483643\n",
      "351 epoch completed. current SGD loss 4.043353080749512\n",
      "352 epoch completed. current SGD loss 4.040875434875488\n",
      "353 epoch completed. current SGD loss 4.038401126861572\n",
      "354 epoch completed. current SGD loss 4.035930633544922\n",
      "355 epoch completed. current SGD loss 4.033463001251221\n",
      "356 epoch completed. current SGD loss 4.030998706817627\n",
      "357 epoch completed. current SGD loss 4.028536796569824\n",
      "358 epoch completed. current SGD loss 4.026078701019287\n",
      "359 epoch completed. current SGD loss 4.023623943328857\n",
      "360 epoch completed. current SGD loss 4.021172046661377\n",
      "361 epoch completed. current SGD loss 4.0187225341796875\n",
      "362 epoch completed. current SGD loss 4.016277313232422\n",
      "363 epoch completed. current SGD loss 4.013835430145264\n",
      "364 epoch completed. current SGD loss 4.0113959312438965\n",
      "365 epoch completed. current SGD loss 4.0089592933654785\n",
      "366 epoch completed. current SGD loss 4.006525993347168\n",
      "367 epoch completed. current SGD loss 4.004096984863281\n",
      "368 epoch completed. current SGD loss 4.001669883728027\n",
      "369 epoch completed. current SGD loss 3.9992456436157227\n",
      "370 epoch completed. current SGD loss 3.996825695037842\n",
      "371 epoch completed. current SGD loss 3.9944088459014893\n",
      "372 epoch completed. current SGD loss 3.9919967651367188\n",
      "373 epoch completed. current SGD loss 3.9895875453948975\n",
      "374 epoch completed. current SGD loss 3.987180233001709\n",
      "375 epoch completed. current SGD loss 3.9847772121429443\n",
      "376 epoch completed. current SGD loss 3.9823782444000244\n",
      "377 epoch completed. current SGD loss 3.9799818992614746\n",
      "378 epoch completed. current SGD loss 3.977588176727295\n",
      "379 epoch completed. current SGD loss 3.9751968383789062\n",
      "380 epoch completed. current SGD loss 3.972808361053467\n",
      "381 epoch completed. current SGD loss 3.9704229831695557\n",
      "382 epoch completed. current SGD loss 3.968041181564331\n",
      "383 epoch completed. current SGD loss 3.9656617641448975\n",
      "384 epoch completed. current SGD loss 3.963284730911255\n",
      "385 epoch completed. current SGD loss 3.9609110355377197\n",
      "386 epoch completed. current SGD loss 3.958540678024292\n",
      "387 epoch completed. current SGD loss 3.9561736583709717\n",
      "388 epoch completed. current SGD loss 3.9538090229034424\n",
      "389 epoch completed. current SGD loss 3.9514479637145996\n",
      "390 epoch completed. current SGD loss 3.949089288711548\n",
      "391 epoch completed. current SGD loss 3.9467344284057617\n",
      "392 epoch completed. current SGD loss 3.9443814754486084\n",
      "393 epoch completed. current SGD loss 3.9420320987701416\n",
      "394 epoch completed. current SGD loss 3.939685821533203\n",
      "395 epoch completed. current SGD loss 3.937342882156372\n",
      "396 epoch completed. current SGD loss 3.9350030422210693\n",
      "397 epoch completed. current SGD loss 3.932666301727295\n",
      "398 epoch completed. current SGD loss 3.9303324222564697\n",
      "399 epoch completed. current SGD loss 3.928001642227173\n",
      "400 epoch completed. current SGD loss 3.9256742000579834\n",
      "401 epoch completed. current SGD loss 3.9233484268188477\n",
      "402 epoch completed. current SGD loss 3.9210259914398193\n",
      "403 epoch completed. current SGD loss 3.918706178665161\n",
      "404 epoch completed. current SGD loss 3.9163904190063477\n",
      "405 epoch completed. current SGD loss 3.9140775203704834\n",
      "406 epoch completed. current SGD loss 3.9117677211761475\n",
      "407 epoch completed. current SGD loss 3.9094603061676025\n",
      "408 epoch completed. current SGD loss 3.9071550369262695\n",
      "409 epoch completed. current SGD loss 3.904853582382202\n",
      "410 epoch completed. current SGD loss 3.902554988861084\n",
      "411 epoch completed. current SGD loss 3.900259017944336\n",
      "412 epoch completed. current SGD loss 3.8979647159576416\n",
      "413 epoch completed. current SGD loss 3.8956754207611084\n",
      "414 epoch completed. current SGD loss 3.89338755607605\n",
      "415 epoch completed. current SGD loss 3.8911032676696777\n",
      "416 epoch completed. current SGD loss 3.888822078704834\n",
      "417 epoch completed. current SGD loss 3.886542558670044\n",
      "418 epoch completed. current SGD loss 3.8842668533325195\n",
      "419 epoch completed. current SGD loss 3.881993532180786\n",
      "420 epoch completed. current SGD loss 3.8797225952148438\n",
      "421 epoch completed. current SGD loss 3.8774545192718506\n",
      "422 epoch completed. current SGD loss 3.8751890659332275\n",
      "423 epoch completed. current SGD loss 3.872927188873291\n",
      "424 epoch completed. current SGD loss 3.870666980743408\n",
      "425 epoch completed. current SGD loss 3.868410587310791\n",
      "426 epoch completed. current SGD loss 3.8661575317382812\n",
      "427 epoch completed. current SGD loss 3.863906145095825\n",
      "428 epoch completed. current SGD loss 3.8616597652435303\n",
      "429 epoch completed. current SGD loss 3.859416961669922\n",
      "430 epoch completed. current SGD loss 3.857175350189209\n",
      "431 epoch completed. current SGD loss 3.8549375534057617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 epoch completed. current SGD loss 3.8527016639709473\n",
      "433 epoch completed. current SGD loss 3.8504698276519775\n",
      "434 epoch completed. current SGD loss 3.848240613937378\n",
      "435 epoch completed. current SGD loss 3.846012830734253\n",
      "436 epoch completed. current SGD loss 3.8437881469726562\n",
      "437 epoch completed. current SGD loss 3.841566324234009\n",
      "438 epoch completed. current SGD loss 3.839348077774048\n",
      "439 epoch completed. current SGD loss 3.837132692337036\n",
      "440 epoch completed. current SGD loss 3.8349194526672363\n",
      "441 epoch completed. current SGD loss 3.8327085971832275\n",
      "442 epoch completed. current SGD loss 3.830500364303589\n",
      "443 epoch completed. current SGD loss 3.8282954692840576\n",
      "444 epoch completed. current SGD loss 3.8260927200317383\n",
      "445 epoch completed. current SGD loss 3.8238935470581055\n",
      "446 epoch completed. current SGD loss 3.82169771194458\n",
      "447 epoch completed. current SGD loss 3.8195040225982666\n",
      "448 epoch completed. current SGD loss 3.8173134326934814\n",
      "449 epoch completed. current SGD loss 3.815124750137329\n",
      "450 epoch completed. current SGD loss 3.8129396438598633\n",
      "451 epoch completed. current SGD loss 3.8107569217681885\n",
      "452 epoch completed. current SGD loss 3.8085758686065674\n",
      "453 epoch completed. current SGD loss 3.806398630142212\n",
      "454 epoch completed. current SGD loss 3.8042242527008057\n",
      "455 epoch completed. current SGD loss 3.802053451538086\n",
      "456 epoch completed. current SGD loss 3.7998836040496826\n",
      "457 epoch completed. current SGD loss 3.797717809677124\n",
      "458 epoch completed. current SGD loss 3.7955551147460938\n",
      "459 epoch completed. current SGD loss 3.7933928966522217\n",
      "460 epoch completed. current SGD loss 3.7912352085113525\n",
      "461 epoch completed. current SGD loss 3.789079189300537\n",
      "462 epoch completed. current SGD loss 3.786926031112671\n",
      "463 epoch completed. current SGD loss 3.784775495529175\n",
      "464 epoch completed. current SGD loss 3.782628059387207\n",
      "465 epoch completed. current SGD loss 3.780482530593872\n",
      "466 epoch completed. current SGD loss 3.7783403396606445\n",
      "467 epoch completed. current SGD loss 3.7762012481689453\n",
      "468 epoch completed. current SGD loss 3.774064064025879\n",
      "469 epoch completed. current SGD loss 3.7719295024871826\n",
      "470 epoch completed. current SGD loss 3.7697982788085938\n",
      "471 epoch completed. current SGD loss 3.7676684856414795\n",
      "472 epoch completed. current SGD loss 3.7655417919158936\n",
      "473 epoch completed. current SGD loss 3.7634174823760986\n",
      "474 epoch completed. current SGD loss 3.761296510696411\n",
      "475 epoch completed. current SGD loss 3.7591779232025146\n",
      "476 epoch completed. current SGD loss 3.7570621967315674\n",
      "477 epoch completed. current SGD loss 3.7549493312835693\n",
      "478 epoch completed. current SGD loss 3.752838611602783\n",
      "479 epoch completed. current SGD loss 3.7507307529449463\n",
      "480 epoch completed. current SGD loss 3.748626232147217\n",
      "481 epoch completed. current SGD loss 3.746523380279541\n",
      "482 epoch completed. current SGD loss 3.7444236278533936\n",
      "483 epoch completed. current SGD loss 3.7423255443573\n",
      "484 epoch completed. current SGD loss 3.7402307987213135\n",
      "485 epoch completed. current SGD loss 3.738137722015381\n",
      "486 epoch completed. current SGD loss 3.7360477447509766\n",
      "487 epoch completed. current SGD loss 3.7339603900909424\n",
      "488 epoch completed. current SGD loss 3.7318742275238037\n",
      "489 epoch completed. current SGD loss 3.7297918796539307\n",
      "490 epoch completed. current SGD loss 3.7277119159698486\n",
      "491 epoch completed. current SGD loss 3.7256343364715576\n",
      "492 epoch completed. current SGD loss 3.7235591411590576\n",
      "493 epoch completed. current SGD loss 3.7214865684509277\n",
      "494 epoch completed. current SGD loss 3.719417095184326\n",
      "495 epoch completed. current SGD loss 3.717348098754883\n",
      "496 epoch completed. current SGD loss 3.715282917022705\n",
      "497 epoch completed. current SGD loss 3.7132198810577393\n",
      "498 epoch completed. current SGD loss 3.71116042137146\n",
      "499 epoch completed. current SGD loss 3.7091028690338135\n",
      "500 epoch completed. current SGD loss 3.7070484161376953\n",
      "--- 9952.077526330948 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#with open('./log.txt', 'w') as f:    \n",
    "    #sys.stdout = f\n",
    "print(\"Experiment taken on : \",time.strftime(\"%c\"))\n",
    "start_time = time.time()\n",
    "main()\n",
    "end_time = (time.time() - start_time)\n",
    "print(\"--- %s seconds ---\" % end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
